{
 "metadata": {
  "language": "Julia",
  "name": "",
  "signature": "sha256:dba6adc21ad93fb831e22b974a78257167b7301242036b84f5c636c6c8adc2b4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using MTNCAR, NeuralNetworkDeterministicPolicyGradient, PyPlot"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#cast problem functions in the correct form\n",
      "init_ = (rng)->init()\n",
      "getNext_ = (rng,s,a)->nextState(s,a)\n",
      "isEnd_ = isEnd\n",
      "reward_ = reward2\n",
      "\n",
      "#initialize problem\n",
      "gm = GenerativeModel(init_,getNext_,isEnd_,reward_)\n",
      "\n",
      "#initialize solvers\n",
      "actor,critic,param, solver,updater = easyInit(2,[1.],[-1.],100,0.0001,\n",
      "cw=0.,cth=0.,ActorLayers=[5.],CriticLayers=[5.],mu=0.9,neuron_type=\"tanh\")\n",
      "\n",
      "#learning rates--note that the actor learning rate is slower than the critic rates\n",
      "alpha_th = 0.0001\n",
      "alpha_v = 0.01\n",
      "alpha_w = 0.01\n",
      "\n",
      "#initialize random number generators\n",
      "trainRNG = MersenneTwister(1)\n",
      "simRNG = MersenneTwister(1000)\n",
      "actRNG = MersenneTwister(500)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#train the actor\n",
      "policy, qs = train(gm,trainRNG,actor, critic,param,solver,updater,\n",
      "               time_horizon=5000,num_episodes=50,eps = 0.25,alpha=[alpha_th; alpha_w;alpha_v],\n",
      "gamma=0.99,natural=true, verbose=true,eps=0.25,natural=true,experience_replay=false)\n",
      "abc=0 #suppress out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#display the average value of each training episode, *should* move towards the initial value function, then begin to noisily go up\n",
      "plot(qs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#simulate the policy\n",
      "nSims = 100\n",
      "R_,hists = runSim(gm,simRNG,actRNG,policy,time_horizon=5000,recordHist=false,nSims=nSims,verbose=true)\n",
      "abc=0 #suppress output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#compare against a heuristic\n",
      "#physics inspired (optimal?) heuristic\n",
      "policy = (rng,s)->s[2]>0.?[1.]:[-1.]\n",
      "R_,hists = runSim(gm,simRNG,actRNG,policy,time_horizon=5000,recordHist=false,nSims=nSims,verbose=true)\n",
      "abc = 0 #suppress output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}